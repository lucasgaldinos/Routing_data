---
config:
  theme: base
  themeVariables:
    primaryColor: '#e0f2f1'
    primaryTextColor: '#00695c'
    primaryBorderColor: '#00897b'
    lineColor: '#26a69a'
    fontFamily: 'JetBrains Mono, Monaco, Consolas, monospace'
    fontSize: 10px
    background: '#fafafa'
  flowchart:
    htmlLabels: true
    curve: basis
    useMaxWidth: true
    diagramPadding: 20
title: Performance Characteristics and Scalability Analysis - TSPLIB95 ETL System
---
flowchart TD
    subgraph performance_metrics["📊 Performance Metrics & Benchmarks"]
        subgraph parsing_performance["📖 Parsing Performance"]
            parse_benchmarks["⏱️ Parsing Benchmarks<br/>File Size vs Processing Time<br/>┌─ Small files (<1MB): ~50ms<br/>├─ Medium files (1-10MB): ~200-500ms<br/>├─ Large files (10-100MB): ~2-15s<br/>├─ Very large files (>100MB): ~30s+<br/>└─ Bottlenecks:<br/>   ├─ Regex parsing: O(n) file scanning<br/>   ├─ String operations: Memory allocation<br/>   └─ Validation: Coordinate range checks<br/><br/>📈 Scaling Characteristics:<br/>• Linear with file size<br/>• Memory usage: ~3x file size peak<br/>• CPU bound for parsing phase<br/>• I/O bound for very large files"]
            
            parse_complexity["🧮 Computational Complexity<br/>Algorithm Analysis<br/>┌─ File scanning: O(n) n=file_size<br/>├─ Node parsing: O(d) d=dimension<br/>├─ Edge parsing: O(e) e=explicit_edges<br/>├─ Coordinate validation: O(d)<br/>└─ Index conversion: O(d + e)<br/><br/>Critical Path:<br/>• EXPLICIT edge types: O(d²) avoided<br/>• Coordinate types: O(d) preferred<br/>• Memory allocation: Major bottleneck<br/>• Garbage collection: 10-20% overhead"]
        end
        
        subgraph database_performance["🗄️ Database Performance"]
            db_benchmarks["💾 Database Operation Benchmarks<br/>DuckDB Performance Characteristics<br/>┌─ Single problem insert: ~5-50ms<br/>├─ Batch insert (100 problems): ~200-800ms<br/>├─ Node bulk insert (10k nodes): ~100ms<br/>├─ Query simple problem: ~1ms<br/>├─ Complex analytics query: ~50-500ms<br/>└─ Database size growth:<br/>   ├─ 1k problems: ~10MB<br/>   ├─ 10k problems: ~100MB<br/>   └─ 100k problems: ~1GB<br/><br/>🚀 Optimization Features:<br/>• Columnar compression: ~5x reduction<br/>• Memory-mapped I/O: Fast random access<br/>• Vectorized operations: SIMD acceleration<br/>• Parallel query execution: Multi-core usage"]
            
            db_scaling["📈 Database Scaling Behavior<br/>Collection Size vs Performance<br/>┌─ Query time scaling:<br/>│  ├─ Simple filters: O(log n) with indexes<br/>│  ├─ Aggregations: O(n) but vectorized<br/>│  ├─ Joins: O(n×m) optimized by planner<br/>│  └─ Full table scans: O(n) columnar<br/>├─ Insert scaling:<br/>│  ├─ Single inserts: O(log n)<br/>│  ├─ Bulk inserts: ~O(1) amortized<br/>│  └─ Index maintenance: O(log n)<br/>└─ Memory usage:<br/>   ├─ Working set: ~10% of database size<br/>   ├─ Query cache: Adaptive size<br/>   └─ Connection overhead: ~1MB per worker"]
        end
        
        subgraph parallel_performance["👥 Parallel Processing Performance"]
            worker_scaling["🔀 Worker Scaling Analysis<br/>Performance vs Worker Count<br/>┌─ Optimal workers: ~CPU_cores<br/>├─ Linear scaling: Up to 8 workers<br/>├─ Diminishing returns: >8 workers<br/>├─ Overhead factors:<br/>│  ├─ Process creation: ~50ms<br/>│  ├─ IPC communication: ~1ms per message<br/>│  ├─ Database connections: ~10ms setup<br/>│  └─ Context switching: ~0.1ms<br/>└─ Bottleneck analysis:<br/>   ├─ I/O bound: Limited by disk speed<br/>   ├─ Memory bound: Limited by RAM<br/>   ├─ CPU bound: Limited by cores<br/>   └─ Database bound: Connection pool size"]
            
            batch_optimization["📦 Batch Size Optimization<br/>Throughput vs Batch Size<br/>┌─ Small batches (1-10 files):<br/>│  ├─ Low latency: Fast feedback<br/>│  ├─ High overhead: Process creation<br/>│  └─ Poor throughput: ~10 files/sec<br/>├─ Medium batches (50-200 files):<br/>│  ├─ Balanced trade-off<br/>│  ├─ Good throughput: ~100 files/sec<br/>│  └─ Reasonable memory usage<br/>├─ Large batches (500+ files):<br/>│  ├─ High throughput: ~200 files/sec<br/>│  ├─ High memory usage: Risk of OOM<br/>│  └─ Poor error isolation<br/>└─ Dynamic adjustment:<br/>   ├─ Monitor memory usage<br/>   ├─ Adjust based on file sizes<br/>   └─ Error rate feedback"]
        end
    end
    
    subgraph memory_analysis["🧠 Memory Usage Analysis"]
        subgraph memory_patterns["📊 Memory Usage Patterns"]
            parsing_memory["🔍 Parsing Memory Profile<br/>Memory Allocation Patterns<br/>┌─ File loading: ~2x file size<br/>├─ TSPLIB object creation: ~3x file size<br/>├─ Validation structures: ~1x file size<br/>├─ Transformation buffers: ~2x file size<br/>└─ Peak usage: ~8x file size<br/><br/>Memory Lifecycle:<br/>• Initial allocation: File read buffer<br/>• Parse phase: Peak memory usage<br/>• Transform phase: Intermediate structures<br/>• Output phase: Database/JSON buffers<br/>• Cleanup: Explicit garbage collection<br/><br/>Optimization Opportunities:<br/>• Streaming large files<br/>• Reference sharing for immutable data<br/>• Object pooling for frequent allocations"]
            
            worker_memory["👷 Per-Worker Memory Usage<br/>Multi-Process Memory Model<br/>┌─ Base worker overhead: ~50MB<br/>├─ Parser instances: ~20MB<br/>├─ Database connection: ~10MB<br/>├─ Processing buffers: Variable<br/>├─ Peak per small file: ~100MB<br/>├─ Peak per large file: ~1GB+<br/>└─ Total system memory:<br/>   └─ N_workers × peak_per_worker<br/><br/>Memory Isolation Benefits:<br/>• Worker crash isolation<br/>• Independent GC cycles<br/>• Parallel memory allocation<br/>• No shared state contention<br/><br/>Memory Monitoring:<br/>• RSS tracking per worker<br/>• Peak usage alerts<br/>• Automatic batch size adjustment"]
        end
        
        subgraph memory_optimization["⚡ Memory Optimization Strategies"]
            streaming_processing["🌊 Streaming Processing<br/>Large File Handling<br/>┌─ File size threshold: 100MB<br/>├─ Chunk-based processing: 10MB chunks<br/>├─ Memory-mapped file access<br/>├─ Lazy evaluation of sections<br/>├─ Incremental validation<br/>└─ Streaming database inserts<br/><br/>Benefits:<br/>• Constant memory usage<br/>• Scalable to arbitrary file sizes<br/>• Reduced GC pressure<br/>• Better cache locality<br/><br/>Trade-offs:<br/>• Increased complexity<br/>• Potential performance overhead<br/>• Limited random access<br/>• More complex error handling"]
            
            memory_pooling["🏊 Memory Pool Management<br/>Allocation Optimization<br/>┌─ Object pools for frequent types:<br/>│  ├─ Node objects: Pre-allocated<br/>│  ├─ Edge objects: Reusable<br/>│  └─ String buffers: Size classes<br/>├─ Memory arena allocation:<br/>│  ├─ Batch-level arenas<br/>│  ├─ Automatic cleanup<br/>│  └─ Reduced fragmentation<br/>└─ Reference counting:<br/>   ├─ Shared coordinate data<br/>   ├─ Immutable problem metadata<br/>   └─ Copy-on-write semantics<br/><br/>Performance Impact:<br/>• 20-30% allocation speedup<br/>• Reduced GC overhead<br/>• Better memory locality<br/>• Lower fragmentation"]
        end
    end
    
    subgraph scalability_limits["📏 Scalability Limits & Boundaries"]
        subgraph current_limits["🚧 Current System Limits"]
            file_limits["📁 File-Level Limits<br/>Individual File Constraints<br/>┌─ Maximum file size: 1GB (configurable)<br/>├─ Maximum dimension: 1M nodes<br/>├─ Maximum edges: 10M explicit edges<br/>├─ Memory per file: 8GB peak<br/>├─ Processing time: 1 hour timeout<br/>└─ Practical limits observed:<br/>   ├─ TSP files: Up to 100k nodes<br/>   ├─ VRP files: Up to 10k nodes<br/>   ├─ EXPLICIT matrices: Up to 5k×5k<br/>   └─ Performance degradation: >50k nodes<br/><br/>Bottleneck Analysis:<br/>• Memory: Primary constraint<br/>• I/O: Secondary for large files<br/>• CPU: Rarely limiting factor<br/>• Database: Scales well with size"]
            
            system_limits["🖥️ System-Level Limits<br/>Hardware and OS Constraints<br/>┌─ Maximum workers: CPU cores (typically 8-16)<br/>├─ Memory limit: 50% of system RAM<br/>├─ Database size: 1TB+ (DuckDB limit)<br/>├─ File descriptors: OS limit (1024+)<br/>├─ Concurrent connections: 100+ per database<br/>└─ Network storage considerations:<br/>   ├─ NFS: Additional I/O latency<br/>   ├─ S3/Cloud: Network bandwidth limits<br/>   └─ Local SSD: Optimal performance<br/><br/>Operating System Factors:<br/>• Virtual memory management<br/>• Process scheduling overhead<br/>• I/O subsystem efficiency<br/>• Network stack performance"]
        end
        
        subgraph scaling_strategies["🚀 Scaling Strategies"]
            vertical_scaling["⬆️ Vertical Scaling (Scale Up)<br/>Single-Machine Optimization<br/>┌─ Hardware upgrades:<br/>│  ├─ More RAM: Handle larger files<br/>│  ├─ More CPU cores: More workers<br/>│  ├─ Faster storage: NVMe SSDs<br/>│  └─ Network upgrades: For remote data<br/>├─ Software optimization:<br/>│  ├─ Memory management tuning<br/>│  ├─ Database configuration<br/>│  ├─ Operating system tuning<br/>│  └─ JIT compilation benefits<br/>└─ Practical limits:<br/>   ├─ Cost effectiveness: Diminishing returns<br/>   ├─ Hardware availability: High-end limits<br/>   └─ Single point of failure<br/><br/>Recommended Configuration:<br/>• 64GB+ RAM for large collections<br/>• 16+ CPU cores for parallel processing<br/>• NVMe SSD for database storage<br/>• 10GbE for network storage"]
            
            horizontal_scaling["➡️ Horizontal Scaling (Scale Out)<br/>Multi-Machine Distribution<br/>┌─ File-level distribution:<br/>│  ├─ Partition files across machines<br/>│  ├─ Independent processing<br/>│  ├─ Result aggregation required<br/>│  └─ Coordination overhead<br/>├─ Database sharding:<br/>│  ├─ Partition by problem type<br/>│  ├─ Partition by size ranges<br/>│  ├─ Cross-shard queries complex<br/>│  └─ Consistency challenges<br/>└─ Implementation approaches:<br/>   ├─ Container orchestration (K8s)<br/>   ├─ Message queue coordination<br/>   ├─ Distributed file systems<br/>   └─ Cloud-native architectures<br/><br/>Trade-offs:<br/>• Complexity vs. scalability<br/>• Consistency vs. partition tolerance<br/>• Cost vs. performance benefits"]
        end
    end
    
    subgraph performance_monitoring["📈 Performance Monitoring & Profiling"]
        subgraph real_time_monitoring["⏱️ Real-Time Performance Monitoring"]
            system_metrics["🖥️ System-Level Metrics<br/>Infrastructure Monitoring<br/>┌─ CPU utilization: Per core and total<br/>├─ Memory usage: RSS, virtual, swap<br/>├─ I/O metrics: Read/write IOPS, throughput<br/>├─ Network metrics: Bandwidth, latency<br/>├─ Disk metrics: Space usage, I/O wait<br/>└─ Process metrics:<br/>   ├─ Worker process health<br/>   ├─ Database connection counts<br/>   ├─ File handle usage<br/>   └─ Error rates by type<br/><br/>Alerting Thresholds:<br/>• Memory usage: >80% system RAM<br/>• CPU usage: >90% sustained<br/>• I/O wait: >20% of CPU time<br/>• Error rate: >5% of processed files<br/>• Worker failures: >1 per hour"]
            
            application_metrics["📊 Application-Level Metrics<br/>Performance Profiling<br/>┌─ Processing rates:<br/>│  ├─ Files per second<br/>│  ├─ Nodes processed per second<br/>│  ├─ Database inserts per second<br/>│  └─ JSON writes per second<br/>├─ Latency metrics:<br/>│  ├─ Parse time per file<br/>│  ├─ Transform time per file<br/>│  ├─ Database insert time<br/>│  └─ End-to-end processing time<br/>├─ Quality metrics:<br/>│  ├─ Parsing success rate<br/>│  ├─ Validation failure rate<br/>│  ├─ Database constraint violations<br/>│  └─ Data consistency checks<br/>└─ Resource utilization:<br/>   ├─ Memory efficiency ratios<br/>   ├─ CPU usage distribution<br/>   └─ I/O wait time analysis"]
        end
        
        subgraph performance_optimization["🔧 Performance Optimization Recommendations"]
            config_tuning["⚙️ Configuration Tuning<br/>Parameter Optimization<br/>┌─ Worker count tuning:<br/>│  ├─ Start with CPU core count<br/>│  ├─ Monitor memory usage<br/>│  ├─ Adjust based on I/O patterns<br/>│  └─ Consider NUMA topology<br/>├─ Batch size optimization:<br/>│  ├─ Start with 100 files per batch<br/>│  ├─ Increase for small files<br/>│  ├─ Decrease for large files<br/>│  └─ Monitor memory pressure<br/>├─ Memory limits:<br/>│  ├─ Per-worker: 2GB default<br/>│  ├─ Total system: 80% of RAM<br/>│  ├─ Database cache: 25% of RAM<br/>│  └─ Emergency reserves: 20% of RAM<br/>└─ Database configuration:<br/>   ├─ Memory allocation<br/>   ├─ Connection pool size<br/>   ├─ Query timeout settings<br/>   └─ Index maintenance schedule"]
            
            hardware_recommendations["🖥️ Hardware Optimization<br/>System Configuration<br/>┌─ CPU recommendations:<br/>│  ├─ High core count (16+ cores)<br/>│  ├─ Good single-thread performance<br/>│  ├─ Large L3 cache (20MB+)<br/>│  └─ Modern instruction sets (AVX2+)<br/>├─ Memory recommendations:<br/>│  ├─ Large capacity (64GB+)<br/>│  ├─ High bandwidth (DDR4-3200+)<br/>│  ├─ Low latency modules<br/>│  └─ ECC for data integrity<br/>├─ Storage recommendations:<br/>│  ├─ NVMe SSD for database<br/>│  ├─ High IOPS capability<br/>│  ├─ Adequate capacity for growth<br/>│  └─ RAID for redundancy<br/>└─ Network considerations:<br/>   ├─ High bandwidth (10GbE+)<br/>   ├─ Low latency connections<br/>   ├─ Dedicated data networks<br/>   └─ Redundant connections"]
        end
    end
    
    %% Performance flow connections
    parse_benchmarks --> db_benchmarks
    parse_complexity --> db_scaling
    db_benchmarks --> worker_scaling
    db_scaling --> batch_optimization
    
    parsing_memory --> worker_memory
    worker_memory --> streaming_processing
    streaming_processing --> memory_pooling
    
    file_limits --> system_limits
    system_limits --> vertical_scaling
    vertical_scaling --> horizontal_scaling
    
    system_metrics --> application_metrics
    application_metrics --> config_tuning
    config_tuning --> hardware_recommendations
    
    classDef performanceMetric fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef memoryAnalysis fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef scalabilityLimit fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef monitoring fill:#fce4ec,stroke:#c2185b,stroke-width:2px
    
    class parse_benchmarks,parse_complexity,db_benchmarks,db_scaling,worker_scaling,batch_optimization performanceMetric
    class parsing_memory,worker_memory,streaming_processing,memory_pooling memoryAnalysis
    class file_limits,system_limits,vertical_scaling,horizontal_scaling scalabilityLimit
    class system_metrics,application_metrics,config_tuning,hardware_recommendations monitoring