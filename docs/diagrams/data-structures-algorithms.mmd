---
config:
  theme: base
  themeVariables:
    primaryColor: '#e3f2fd'
    primaryTextColor: '#1565c0'
    primaryBorderColor: '#1976d2'
    lineColor: '#42a5f5'
    fontFamily: 'JetBrains Mono, Monaco, Consolas, monospace'
    fontSize: 10px
    background: '#fafafa'
  flowchart:
    htmlLabels: true
    curve: basis
    useMaxWidth: true
    diagramPadding: 25
title: Data Structures and Memory Layout - TSPLIB95 ETL System
---
flowchart TB
    subgraph memory_layout["Memory Layout & Data Structures"]
        subgraph tsplib_raw["Raw TSPLIB95 Data (Phase 1)"]
            raw_problem["ğŸ“‹ StandardProblem Object<br/>â”Œâ”€ name: str<br/>â”œâ”€ type: str (TSP|VRP|ATSP|...)<br/>â”œâ”€ dimension: int<br/>â”œâ”€ capacity: int | None<br/>â”œâ”€ edge_weight_type: str<br/>â”œâ”€ node_coords: Dict[int, Tuple]<br/>â”œâ”€ demands: Dict[int, int]<br/>â”œâ”€ depots: List[int]<br/>â”œâ”€ edge_weights: Dict[Tuple, float]<br/>â””â”€ tours: List[List[int]]"]
            
            raw_memory["Memory Characteristics:<br/>â€¢ 1-based indexing (TSPLIB standard)<br/>â€¢ O(n) for coordinates<br/>â€¢ O(nÂ²) for explicit edges (avoided)<br/>â€¢ Dict-based sparse storage<br/>â€¢ Lazy edge computation"]
        end
        
        subgraph transformed_data["Transformed Data (Phase 2)"]
            norm_structure["ğŸ”„ Normalized Structure<br/>â”Œâ”€ problem_data: Dict<br/>â”‚  â”œâ”€ id: int (auto-generated)<br/>â”‚  â”œâ”€ name: str<br/>â”‚  â”œâ”€ type: str<br/>â”‚  â”œâ”€ dimension: int<br/>â”‚  â””â”€ metadata: Dict<br/>â”œâ”€ nodes: List[Dict]<br/>â”‚  â”œâ”€ node_id: int (0-based)<br/>â”‚  â”œâ”€ x, y, z: float | None<br/>â”‚  â”œâ”€ demand: int<br/>â”‚  â””â”€ is_depot: bool<br/>â”œâ”€ edges: List[Dict] (EXPLICIT only)<br/>â”‚  â”œâ”€ from_node: int (0-based)<br/>â”‚  â”œâ”€ to_node: int (0-based)<br/>â”‚  â””â”€ weight: float<br/>â””â”€ tours: List[List[int]] (0-based)"]
            
            transform_memory["Memory Optimization:<br/>â€¢ 0-based indexing (database-friendly)<br/>â€¢ List-based for fast iteration<br/>â€¢ Only store edges for EXPLICIT types<br/>â€¢ Coordinate-based distance on-demand<br/>â€¢ Batch processing for large files"]
        end
        
        subgraph db_storage["Database Storage (Phase 3)"]
            db_tables["ğŸ’¾ DuckDB Tables<br/>â”Œâ”€ problems: Row-oriented<br/>â”‚  â””â”€ Clustered by ID<br/>â”œâ”€ nodes: Column-optimized<br/>â”‚  â”œâ”€ Indexed by (problem_id, node_id)<br/>â”‚  â””â”€ Spatial index for coordinates<br/>â”œâ”€ edges: Compressed storage<br/>â”‚  â”œâ”€ Indexed by (problem_id, from_node)<br/>â”‚  â””â”€ Only for EXPLICIT weights<br/>â”œâ”€ tours: JSON column storage<br/>â”‚  â””â”€ Indexed by problem_id<br/>â””â”€ file_tracking: Metadata<br/>     â”œâ”€ File hash for change detection<br/>     â””â”€ Processing status tracking"]
            
            db_perf["Performance Characteristics:<br/>â€¢ Columnar compression (5-10x)<br/>â€¢ Parallel query execution<br/>â€¢ Memory-mapped I/O<br/>â€¢ Automatic index optimization<br/>â€¢ Sub-second analytics queries"]
        end
    end
    
    subgraph algorithms["Core Algorithms & Complexity"]
        subgraph parsing_algo["Parsing Algorithm"]
            parse_steps["ğŸ“Š Parsing Process<br/>â”Œâ”€ 1. File validation: O(1)<br/>â”œâ”€ 2. Header parsing: O(k) k=fields<br/>â”œâ”€ 3. Section parsing: O(n+e)<br/>â”‚   â”œâ”€ COORD_SECTION: O(n)<br/>â”‚   â”œâ”€ DEMAND_SECTION: O(n)<br/>â”‚   â”œâ”€ DEPOT_SECTION: O(d)<br />â”‚   â””â”€ EDGE_WEIGHT_SECTION: O(e)<br/>â”œâ”€ 4. Data validation: O(n)<br/>â””â”€ 5. Structure creation: O(n)"]
            
            parse_complex["Overall Complexity:<br/>â€¢ Time: O(n + e) where n=nodes, e=edges<br/>â€¢ Space: O(n + e)<br/>â€¢ Critical: Avoid O(nÂ²) edge generation<br/>â€¢ Memory-efficient streaming for large files"]
        end
        
        subgraph transform_algo["Transformation Algorithm"]
            transform_steps["âš¡ Transform Process<br/>â”Œâ”€ 1. Index conversion: O(n)<br/>â”‚   â””â”€ Map: i â†’ i-1 (1-based to 0-based)<br/>â”œâ”€ 2. Node normalization: O(n)<br/>â”‚   â”œâ”€ Coordinate validation<br/>â”‚   â”œâ”€ Demand assignment<br/>â”‚   â””â”€ Depot flag setting<br/>â”œâ”€ 3. Edge processing: O(e) or O(1)<br/>â”‚   â”œâ”€ EXPLICIT: Store provided edges<br/>â”‚   â””â”€ COORDINATE: Skip (compute on-demand)<br/>â””â”€ 4. Tour normalization: O(tÃ—m)<br/>     â””â”€ t=tours, m=avg_tour_length"]
            
            transform_memory["Memory Management:<br/>â€¢ Streaming transformation<br/>â€¢ Batch processing (configurable)<br/>â€¢ Garbage collection friendly<br/>â€¢ Copy-on-write for large structures"]
        end
        
        subgraph distance_algo["Distance Computation"]
            distance_types["ğŸ“ Distance Functions<br/>â”Œâ”€ EUC_2D: âˆš[(xâ‚‚-xâ‚)Â² + (yâ‚‚-yâ‚)Â²]<br/>â”œâ”€ MAN_2D: |xâ‚‚-xâ‚| + |yâ‚‚-yâ‚|<br/>â”œâ”€ CEIL_2D: âŒˆEUC_2DâŒ‰<br/>â”œâ”€ GEO: Haversine formula<br/>â”‚   â””â”€ 6378.388 Ã— arccos(sin(Ï†â‚)sin(Ï†â‚‚) +<br/>â”‚       cos(Ï†â‚)cos(Ï†â‚‚)cos(Î”Î»))<br/>â”œâ”€ ATT: âˆš[(xâ‚‚-xâ‚)Â² + (yâ‚‚-yâ‚)Â²]/10<br/>â””â”€ EXPLICIT: Pre-stored matrix lookup"]
            
            distance_perf["Performance:<br/>â€¢ EUC_2D/MAN_2D: ~100ns per calculation<br/>â€¢ GEO: ~500ns (trigonometric functions)<br/>â€¢ EXPLICIT: ~50ns (array lookup)<br/>â€¢ Vectorization possible for batch calculations"]
        end
    end
    
    subgraph parallel_arch["Parallel Processing Architecture"]
        subgraph worker_model["Worker Process Model"]
            worker_design["ğŸ‘¥ Multi-Process Design<br/>â”Œâ”€ Main Process:<br/>â”‚  â”œâ”€ File discovery & batching<br/>â”‚  â”œâ”€ Result aggregation<br/>â”‚  â””â”€ Progress monitoring<br/>â”œâ”€ Worker Processes (N):<br/>â”‚  â”œâ”€ Independent parser instances<br/>â”‚  â”œâ”€ Local database connections<br/>â”‚  â”œâ”€ Memory limit monitoring<br/>â”‚  â””â”€ Error isolation<br/>â””â”€ Batch Coordinator:<br/>   â”œâ”€ Load balancing<br/>   â”œâ”€ Failure recovery<br/>   â””â”€ Resource management"]
            
            worker_sync["Synchronization:<br/>â€¢ File-level locking (filesystem)<br/>â€¢ Database connection pooling<br/>â€¢ Shared progress counter (atomic)<br/>â€¢ Result queue (thread-safe)<br/>â€¢ Graceful shutdown handling"]
        end
        
        subgraph batch_strategy["Batch Processing Strategy"]
            batch_logic["ğŸ“¦ Batching Logic<br/>â”Œâ”€ File Size Estimation:<br/>â”‚  â”œâ”€ Small files (<1MB): batch=200<br/>â”‚  â”œâ”€ Medium files (1-10MB): batch=50<br/>â”‚  â””â”€ Large files (>10MB): batch=10<br/>â”œâ”€ Memory-Based Batching:<br/>â”‚  â”œâ”€ Monitor RSS per worker<br/>â”‚  â”œâ”€ Reduce batch size if memory high<br/>â”‚  â””â”€ Emergency GC if limit exceeded<br/>â””â”€ Dynamic Adjustment:<br/>   â”œâ”€ Performance feedback loop<br/>   â”œâ”€ Error rate monitoring<br/>   â””â”€ Optimal batch size learning"]
            
            batch_perf["Batch Performance:<br/>â€¢ I/O amortization: ~50% reduction<br/>â€¢ Memory locality: Better cache usage<br/>â€¢ Error isolation: Batch-level recovery<br/>â€¢ Progress tracking: Smoother updates"]
        end
    end
    
    subgraph edge_cases["Edge Case Handling"]
        subgraph data_quality["Data Quality Issues"]
            quality_checks["ğŸ” Quality Validation<br/>â”Œâ”€ File Format Issues:<br/>â”‚  â”œâ”€ Missing required sections<br/>â”‚  â”œâ”€ Malformed coordinates<br/>â”‚  â”œâ”€ Inconsistent dimensions<br/>â”‚  â””â”€ Invalid character encoding<br/>â”œâ”€ Data Consistency:<br/>â”‚  â”œâ”€ Node ID gaps or duplicates<br/>â”‚  â”œâ”€ Coordinate range validation<br/>â”‚  â”œâ”€ Demand/capacity consistency<br/>â”‚  â””â”€ Tour feasibility checks<br/>â””â”€ Performance Issues:<br/>   â”œâ”€ Extremely large files (>1GB)<br/>   â”œâ”€ Dense edge matrices<br/>   â””â”€ Memory exhaustion scenarios"]
            
            error_recovery["Error Recovery:<br/>â€¢ Graceful degradation<br/>â€¢ Partial data preservation<br/>â€¢ Detailed error logging<br/>â€¢ Retry with relaxed validation<br/>â€¢ Skip corrupted sections"]
        end
        
        subgraph memory_management["Memory Management"]
            memory_strategy["ğŸ’¾ Memory Strategy<br/>â”Œâ”€ File Size Limits:<br/>â”‚  â”œâ”€ Default: 100MB per file<br/>â”‚  â”œâ”€ Configurable via settings<br/>â”‚  â””â”€ Automatic chunking for larger files<br/>â”œâ”€ Worker Memory Limits:<br/>â”‚  â”œâ”€ RSS monitoring per process<br/>â”‚  â”œâ”€ Configurable limit (default: 2GB)<br/>â”‚  â””â”€ Emergency cleanup procedures<br/>â””â”€ Garbage Collection:<br/>   â”œâ”€ Explicit GC after batch<br/>   â”œâ”€ Reference cycle detection<br/>   â””â”€ Memory pool optimization"]
            
            oom_handling["OOM Prevention:<br/>â€¢ Pre-allocation checks<br/>â€¢ Streaming for large datasets<br/>â€¢ Temporary file spillover<br/>â€¢ Process restart on memory limit<br/>â€¢ User warnings for large files"]
        end
    end
    
    raw_problem --> norm_structure
    norm_structure --> db_tables
    parse_steps --> transform_steps
    transform_steps --> worker_design
    worker_design --> batch_logic
    quality_checks --> error_recovery
    memory_strategy --> oom_handling
    
    classDef rawData fill:#ffecb3,stroke:#ff8f00,stroke-width:2px
    classDef transformedData fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef dbStorage fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef algorithm fill:#fce4ec,stroke:#c2185b,stroke-width:2px
    classDef parallel fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef edgeCase fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    
    class raw_problem,raw_memory rawData
    class norm_structure,transform_memory transformedData
    class db_tables,db_perf dbStorage
    class parse_steps,parse_complex,transform_steps,transform_memory,distance_types,distance_perf algorithm
    class worker_design,worker_sync,batch_logic,batch_perf parallel
    class quality_checks,error_recovery,memory_strategy,oom_handling edgeCase